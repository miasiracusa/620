{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data 620 - Assignment 6<br>July 10, 2019<br>Team 2: <ul> <li>Anthony Munoz</li> <li>Katie Evers</li> <li>Juliann McEachern</li> <li>Mia Siracusa</li></ul>\n",
    "<h1 align=\"center\">\"Document Classification\"</h1>\n",
    "\n",
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  \n",
    "\n",
    "Here is one example of such data:  [UCI Machine Learning Repository: Spambase Data Set](http://archive.ics.uci.edu/ml/datasets/Spambase).\n",
    "\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.\n",
    "\n",
    "This assignment is due end of day on Wednesday, July 10th.  You may work in a small team if you want.\n",
    "\n",
    "*NOTE: This is a two week assignment.*\n",
    "\n",
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing packages\n",
    "import pandas as pd, numpy as np, os\n",
    "\n",
    "# nltk packages\n",
    "from nltk import stem; from nltk.corpus import stopwords\n",
    "\n",
    "# sklearn packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Uploading \n",
    "\n",
    "We read the ham and spam data from a csv file in our github repository and relabled the data columns. The shape of our dataframe and a preview of the data can be viewed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                              email\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv\n",
    "df_data = pd.read_csv('https://raw.githubusercontent.com/Anth350z/620/master/Assignments/data/ham_spam_data', \n",
    "                      error_bad_lines=False, delimiter=\"\\t\",header=None)\n",
    "\n",
    "# label columns\n",
    "df_data.columns = ['label','email']\n",
    "\n",
    "# preview data\n",
    "print(\"data shape:\",df_data.shape)\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning (Stemmer/Stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we used the stem and stopword packages from nltk to improve our natural language processing technique. We then created a function to lower the data using the stem and use stopwords for cleaning and organizational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave only the word stem & remove stop words (ie. a, and, the, etc.) \n",
    "stemmer = stem.SnowballStemmer('english')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# create data cleaning function\n",
    "def data_cleaning(data):\n",
    "    data = data.lower()\n",
    "    data = [message_word for message_word  in data.split() if message_word  not in stopwords]\n",
    "    data = \" \".join([stemmer.stem(message_word ) for message_word  in data])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied the function to our dataframe. The results of our data_cleaning can be previewed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point, crazy.. avail bugi n great wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joke wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor... u c alreadi say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf, live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                              email\n",
       "0   ham  go jurong point, crazy.. avail bugi n great wo...\n",
       "1   ham                        ok lar... joke wif u oni...\n",
       "2  spam  free entri 2 wkli comp win fa cup final tkts 2...\n",
       "3   ham          u dun say earli hor... u c alreadi say...\n",
       "4   ham              nah think goe usf, live around though"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['email'] = df_data['email'].apply(data_cleaning)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Test Datasets\n",
    "\n",
    "We further prepared our data by applying a term frequencyâ€“inverse document frequency (TFIDF) vectorizer to our email values. The `TfidfVectorizer` function extracts important features from our corpus. \n",
    "\n",
    "From there, we used the `train_test_split` function from `sklearn` to split our data 80/20 for training and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare and split the data for training/testing purposes \n",
    "email = df_data['email'].values\n",
    "label = df_data['label'].values\n",
    "\n",
    "# TFIDF vectorizer \n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "email = vectorizer.fit_transform(email)\n",
    "\n",
    "# training the data splitting with 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(email,label,test_size=0.2, shuffle=True,random_state=0,  stratify=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "We first used the multinomial Naive Bayes classifier for our prediction model. Our training and test accuracy both faired very well with accuracy above 95% in both cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9796\n",
      "Testing Accuracy: 0.9686\n"
     ]
    }
   ],
   "source": [
    "# set classifier \n",
    "clf = MultinomialNB()\n",
    "\n",
    "# fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "# score accuracy \n",
    "clf_train_accuracy = round(clf.score(X_train, y_train),4)\n",
    "clf_test_accuracy = round(clf.score(X_test, y_test),4)\n",
    "\n",
    "# print accuracy \n",
    "print(\"Training Accuracy: \" + str(clf_train_accuracy))\n",
    "print(\"Testing Accuracy: \" + str(clf_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "Our next attempt used decision trees to classify our ham and spam using the `RandomForestClassifier` function. This method performed extremely well. We observed perfect classification of our training set and saw improvements on our test prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set classifier \n",
    "RDC = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# fit model\n",
    "RDC.fit(X_train,y_train)\n",
    "\n",
    "# make predictions\n",
    "prediction = RDC.predict(X_test)\n",
    "\n",
    "# score accuracy \n",
    "RDC_train_accuracy = round(RDC.score(X_train, y_train),4)\n",
    "RDC_test_accuracy = round(RDC.score(X_test, y_test),4)\n",
    "\n",
    "# print accuracy \n",
    "print(\"Training Accuracy: \" + str(RDC_train_accuracy))\n",
    "print(\"Testing Accuracy: \" + str(RDC_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Our final method used the stochastic gradient descent technique. While our accuracy slightly lowered, we found this method predicted our training data the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set classifier \n",
    "SGD = SGDClassifier(max_iter=1000,tol=0.001)\n",
    "\n",
    "# make predictions\n",
    "SGD.fit(X_train,y_train)\n",
    "\n",
    "# make predictions\n",
    "prediction = SGD.predict(X_test)\n",
    "\n",
    "# score accuracy \n",
    "SGD_train_accuracy = round(SGD.score(X_train, y_train),4)\n",
    "SGD_test_accuracy = round(SGD.score(X_test, y_test),4)\n",
    "\n",
    "# print accuracy \n",
    "print(\"Training Accuracy: \" + str(SGD_train_accuracy))\n",
    "print(\"Testing Accuracy: \" + str(SGD_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. https://www.datacamp.com/community/blog/text-mining-in-r-and-python-tips\n",
    "2. https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "3. https://towardsdatascience.com/spam-or-ham-introduction-to-natural-language-processing-part-2-a0093185aebd\n",
    "4. https://gtraskas.github.io/post/spamit/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
